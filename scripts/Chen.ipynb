{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index de cuduments bien lu\n",
      "index inversé bien lu\n",
      "votre requête ?\n",
      "chomage citoyen culture day department\n",
      "... chomage citoyen culture day department\n",
      "\n",
      "{14: {'chomage': 3, 'citoyen': 1, 'culture': 5}, 22: {'chomage': 1, 'citoyen': 1}, 0: {'citoyen': 1}, 5: {'citoyen': 2}, 10: {'citoyen': 3}, 1: {'culture': 1}, 6: {'culture': 1}, 8: {'culture': 2}, 21: {'culture': 2, 'day': 4}, 24: {'culture': 2, 'day': 3, 'department': 5}, 26: {'culture': 1}, 28: {'culture': 5, 'day': 1}, 3: {'day': 1, 'department': 1}, 13: {'day': 1}, 17: {'day': 2}, 19: {'day': 5, 'department': 2}, 29: {'day': 2}, 12: {'department': 1}}\n",
      "18 documents ont été trouvés!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------------------\n",
    "#   python3 requete-v1.py\n",
    "#\n",
    "#   Traitement d'une requête :\n",
    "#       0. lecture fichier des index (inversé & documents)\n",
    "#       1. lecture de la requête\n",
    "#       2. extraction de ses tokens \n",
    "#       - lemmatisation\n",
    "#       - normalisation \n",
    "#       3. recherche des documents correspondants dans l'index\n",
    "#           par comptage des matchs\n",
    "#       4. affichage des résultats\n",
    "# \n",
    "#   remarque :  ne donne pas d'info sur les termes trouvés\n",
    "# -------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import pickle \n",
    "import json\n",
    "from fonctions_index import litTexteDuFichier, ecritTexteDansUnFichier, lemmatiseTexte, normaliseTokens, normaliseTokensRequete, lemmatiseTokenRequete\n",
    "\n",
    "PARSEUR = 'TALISMANE'\n",
    "   \n",
    "fiIndex = \"./_index/indexInverse\"\n",
    "fiDocs = \"./_index/indexDocuments\"\n",
    "doCorpus = \"../corpus/miniCorpus/FR/\"\n",
    "corpus_sansBalise = \"../corpus/sans_balises\"\n",
    "\n",
    "fiTxt = \"./_log/tempo.txt\" \n",
    "\n",
    "fiLog = \"./_log/requete.log\"\n",
    "log = \"\"\n",
    "\n",
    "\n",
    "indexDocuments = dict()\n",
    "indexInverse = dict ()\n",
    "\n",
    "# 0. lecture des index\n",
    "if os.path.isfile (fiDocs) :\n",
    "    with open(fiDocs, 'rb') as FI :\n",
    "        #indexDocuments = pickle.load(FI)\n",
    "        indexDocuments = json.load(FI)\n",
    "    print (\"index de cuduments bien lu\")\n",
    "\n",
    "if os.path.isfile (fiIndex) :\n",
    "    with open(fiIndex, 'rb') as FI :\n",
    "        #indexInverse = pickle.load(FI)\n",
    "        indexInverse = json.load(FI)\n",
    "    print (\"index inversé bien lu\")\n",
    " \n",
    "# 1. lecture de la requête (mots clés)\n",
    "print(\"votre requête ?\")\n",
    "requete = input()\n",
    "print(\"...\", requete)\n",
    "log += \"\\nrequête: %s \\n\" % (requete)\n",
    "\n",
    "# 2. extraction de ses tokens (lemmatisation, normalisation)\n",
    "ecritTexteDansUnFichier (requete, fiTxt)   \n",
    "tokens = lemmatiseTokenRequete (fiTxt)\n",
    "termes = normaliseTokensRequete (tokens)\n",
    "log += \"\\ntermes: %s \\n\" % (termes)\n",
    "\n",
    "# 3. recherche des termes dans l'index\n",
    "nbMatch = dict ()\n",
    "for terme in termes:\n",
    "    # pour chaque terme de la requête\n",
    "    if terme in indexInverse:\n",
    "        for liste in indexInverse[terme]:\n",
    "            if nbMatch.__contains__(liste[0]):\n",
    "                nbMatch[liste[0]][terme] = liste[1]\n",
    "            else:\n",
    "                valeur = {terme:liste[1]}\n",
    "                nbMatch[liste[0]] = valeur\n",
    "\n",
    "log += \"\\nrésultat: %s \\n\" % (str (nbMatch) )\n",
    "log += \"\\nresultats détaillées: \\n\"\n",
    "\n",
    "# 4. affichage des résultats\n",
    "print(f\"\\n{nbMatch}\")\n",
    "print(\"{nombre} documents ont été trouvés!\\n\".format(nombre=len(nbMatch)))\n",
    "\n",
    "for doc, nb in nbMatch.items():#sorted(nbMatch.items(), key=lambda item: item[0], reverse=True) :\n",
    "    info_document = indexDocuments[str(doc)]\n",
    "    #print(f\"{info_document} --> {nb}\")\n",
    "\n",
    "    #stocker les infos dans log\n",
    "    nom_fichier = info_document[0]\n",
    "    titre_complet = info_document[1]\n",
    "    if \"\\n\\n\" in titre_complet:\n",
    "        titre_principal = titre_complet[:titre_complet.index(\"\\n\\n\")]\n",
    "        sous_titre = titre_complet[titre_complet.index(\"\\n\\n\")+2:]\n",
    "        log += f\"nom de fichier: {nom_fichier}\\n titre de texte: {titre_principal}\\n sous-titre: {sous_titre}\\n termes trouvés dans ce fichier: {nb}\\n\\n\"\n",
    "    else:\n",
    "        log += f\"nom de fichier: {nom_fichier}\\n titre de texte: {titre_complet}\\n termes trouvés dans ce fichier: {nb}\\n\\n\"\n",
    "\n",
    "#5 calculer les scores avec TF-IDF\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# sauvegarde du log\n",
    "ecritTexteDansUnFichier (log, fiLog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['je', '-ne', '+sais', '-pas']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'je ne sais pas'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mots_list = \"je -ne +sais -pas\"\n",
    "tokens = mots_list.split(\" \")\n",
    "tokens_sans_ponc=[]\n",
    "\n",
    "print(tokens)\n",
    "for i in tokens:\n",
    "    if i[0] == \"-\" or i[0] == \"+\":\n",
    "        tokens_sans_ponc.append(i[1:])\n",
    "    else:\n",
    "        tokens_sans_ponc.append(i)\n",
    "cave = \" \".join(tokens_sans_ponc)\n",
    "cave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "join() takes no keyword arguments",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-eb6a6b4609e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0maa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0maa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"s\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"o\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"i\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"a\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'l'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: join() takes no keyword arguments"
     ]
    }
   ],
   "source": [
    "aa = \"\"\n",
    "aa.join([\"s\",\"o\",\"c\",\"i\",\"a\",'l'],split=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#删除加减号的版本\n",
    "def lemmatiseTokensRequete(fichier):\n",
    "    tokensLemmePos = []\n",
    "    fiParse = fichier + \".par\"\n",
    "\n",
    "    #supprimer le \"+\" et le \"-\" des termes de la requête\n",
    "    requete_termes = litTexteDuFichier(fichier)\n",
    "    termes_avec_ponc = requete_termes.split(\" \")\n",
    "    liste_termes_sans_ponc = []\n",
    "\n",
    "    for terme in termes_avec_ponc:\n",
    "        if terme[0] == \"-\" or terme[0] == \"+\":\n",
    "            liste_termes_sans_ponc.append(terme[1:])\n",
    "        else:\n",
    "            liste_termes_sans_ponc.append(terme)\n",
    "    termes_a_traiter = \" \".join(liste_termes_sans_ponc)\n",
    "\n",
    "    #écrire à nouveau les termes sans ponctuations dans le fichier \"tempo.txt\"\n",
    "    ecritTexteDansUnFichier(termes_a_traiter, fichier)\n",
    "    \n",
    "    # execute TreeTagger\n",
    "    if detect(requete_termes) == \"fr\":\n",
    "        cmd = \"tree-tagger-french\"\n",
    "    else:\n",
    "        cmd = \"tree-tagger-english\"\n",
    "    os.system(\"%s %s > %s\" % (cmd, fichier, fiParse))\n",
    "\n",
    "    # lit le csv produit par treeTagger\n",
    "    with open(fiParse, \"r\") as FI:\n",
    "        for ligne in FI:\n",
    "            ligne = ligne.strip()\n",
    "            defToken = ligne.split(\"\\t\")\n",
    "            # filtre les tokens vides\n",
    "            if len(defToken) >= 3:\n",
    "                if defToken[2] != \"<unknown>\":\n",
    "                    # ajouter les tokens lemmatisés dans la liste \n",
    "                    tokensLemmePos.append(defToken[2])\n",
    "                else:\n",
    "                    tokensLemmePos.append(defToken[0])\n",
    "    # renvoie une liste de tokens (lemme)\n",
    "    return tokensLemmePos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copie de la fonction de \"fonctions_index.py\"\n",
    "def lemmatiseTokenRequete(fichier):\n",
    "    tokensLemmePos = []\n",
    "    fiParse = fichier + \".par\"\n",
    "\n",
    "\n",
    "    # execute TreeTagger\n",
    "    cmd = \"tree-tagger-french\"\n",
    "    os.system(\"%s %s > %s\" % (cmd, fichier, fiParse))\n",
    "    # lit le csv produit par treeTagger\n",
    "    with open(fiParse, \"r\") as FI:\n",
    "        for ligne in FI:\n",
    "            ligne = ligne.strip()\n",
    "            defToken = ligne.split(\"\\t\")\n",
    "            # filtre les tokens vides\n",
    "            if len(defToken) >= 3:\n",
    "                if defToken[2] != \"<unknown>\":\n",
    "                    # ajouter les tokens lemmatisés dans la liste \n",
    "                    tokensLemmePos.append(defToken[2])\n",
    "                else:\n",
    "                    tokensLemmePos.append(defToken[0])\n",
    "    # renvoie une liste de tokens (lemme)\n",
    "    return tokensLemmePos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultats = {14: {'chomage': 3, 'citoyen': 1, 'economie': 3, 'societe': 1}, 22: {'chomage': 1, 'citoyen': 1, 'economie': 2, 'societe': 1}, 0: {'citoyen': 1, 'economie': 1}, 5: {'citoyen': 2, 'societe': 1}, 10: {'citoyen': 3, 'societe': 5}, 4: {'economie': 4, 'societe': 3, 'salaire': 3}, 7: {'economie': 1, 'societe': 1}, 11: {'economie': 4, 'societe': 1}, 15: {'economie': 1, 'societe': 4}, 26: {'economie': 9, 'societe': 1}} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{14: {'chomage': 3, 'citoyen': 1, 'economie': 3, 'societe': 1},\n",
       " 22: {'chomage': 1, 'citoyen': 1, 'economie': 2, 'societe': 1},\n",
       " 0: {'citoyen': 1, 'economie': 1},\n",
       " 5: {'citoyen': 2, 'societe': 1},\n",
       " 10: {'citoyen': 3, 'societe': 5},\n",
       " 4: {'economie': 4, 'societe': 3, 'salaire': 3},\n",
       " 7: {'economie': 1, 'societe': 1},\n",
       " 11: {'economie': 4, 'societe': 1},\n",
       " 15: {'economie': 1, 'societe': 4},\n",
       " 26: {'economie': 9, 'societe': 1}}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "resultats_finals = copy.deepcopy(resultats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{14: {'chomage': 3, 'citoyen': 1, 'economie': 3, 'societe': 1},\n",
       " 22: {'chomage': 1, 'citoyen': 1, 'economie': 2, 'societe': 1},\n",
       " 0: {'citoyen': 1, 'economie': 1},\n",
       " 5: {'citoyen': 2, 'societe': 1},\n",
       " 10: {'citoyen': 3, 'societe': 5},\n",
       " 4: {'economie': 4, 'societe': 3, 'salaire': 3},\n",
       " 7: {'economie': 1, 'societe': 1},\n",
       " 11: {'economie': 4, 'societe': 1},\n",
       " 15: {'economie': 1, 'societe': 4},\n",
       " 26: {'economie': 9, 'societe': 1}}"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultats_finals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "requete = [\"Chômages\",\"citoyens\",\"+Économie\",\"+sociétés\",\"-salaire\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "requete_finale = [\"chomage\",\"citoyen\",\"+economie\",\"+societe\",\"-salaire\"]#7,11,15,26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "10",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-163-42da3ac67da4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdocument\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresultats_finals\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mterme\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrequete_finale\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mterme\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mterme\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresultats_finals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m             \u001b[0mresultats_finals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 10"
     ]
    }
   ],
   "source": [
    "rrr = {}\n",
    "for document in resultats_finals:\n",
    "    for terme in requete_finale:\n",
    "        if terme.startswith(\"-\") and terme[1:] in resultats_finals[document]:\n",
    "            resultats_finals.pop(document)\n",
    "        else:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "ag = list(resultats_finals.items())\n",
    "mots_a_traiter = ag[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'economie': 4, 'societe': 3, 'salaire': 3}"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mots_a_traiter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all(term in mots_a_traiter for term in requete_finale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = (term in mots_a_traiter for term in requete_finale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[False, False, False, False, False]"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[False, False, False, False, False]"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = list(term in mots_a_traiter for term in requete_finale)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "for terme in requete_finale:\n",
    "    for document in resultats_finals:\n",
    "        if (terme[0] == \"+\") and (terme[1:] in resultats_finals[document]):\n",
    "            if document in rrr:\n",
    "                pass\n",
    "            else:\n",
    "                rrr[document] = resultats_finals.get(document)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dfs +dsdfd -sfe +dd\n",
      "['dfs', '+dsdfd', '-sfe', '+dd']\n"
     ]
    }
   ],
   "source": [
    "inputt = input()\n",
    "aa = list(inputt.split(\" \"))\n",
    "print(aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "import treetaggerwrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-241-ae39d607adce>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-241-ae39d607adce>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    texte = extraitTexteDuFichier(fichier, nom_fichier)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "tokensLemmePos = []\n",
    "    texte = extraitTexteDuFichier(fichier, nom_fichier)\n",
    "    if detect(texte) == \"fr\":\n",
    "        tagger = treetaggerwrapper.TreeTagger(TAGLANG=\"fr\")\n",
    "        tags = tagger.tag_text(texte)\n",
    "        lang = \"FR\"\n",
    "    else:\n",
    "        tagger = treetaggerwrapper.TreeTagger(TAGLANG=\"en\")\n",
    "        tags = tagger.tag_text(texte)\n",
    "        lang = \"EN\"\n",
    "    for ligne in tags:\n",
    "        ligne = ligne.strip()\n",
    "        defToken = ligne.split(\"\\t\")\n",
    "        # filtre les tokens vides\n",
    "        if len(defToken) >= 3:\n",
    "            # reduit aux lemmes et pos\n",
    "            token = [defToken[2], defToken[1]]\n",
    "            tokensLemmePos.append(token)\n",
    "    # renvoie une liste de tokens (lemme et pos)\n",
    "    return tokensLemmePos, lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "termes_test = [\"chômages\",\"salaires\",\"citoyenne\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger = treetaggerwrapper.TreeTagger(TAGLANG=\"fr\")\n",
    "aa = tagger.tag_text(termes_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chômages\\tNOM\\tchômage', 'salaires\\tNOM\\tsalaire', 'citoyenne\\tNOM\\tcitoyen']"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in aa:\n",
    "    i = i.strip()\n",
    "    token = i.split(\"\\t\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'chômages salaires citoyenne'"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \" \".join(termes_test)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fr'"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detect(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger = treetaggerwrapper.TreeTagger(TAGLANG=\"fr\")\n",
    "aa = tagger.tag_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chômages\\tNOM\\tchômage', 'salaires\\tNOM\\tsalaire', 'citoyenne\\tNOM\\tcitoyen']"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatiseTermes(liste):\n",
    "    tokensLemme = []\n",
    "    # execute TreeTagger\n",
    "    texte = \" \".join(liste)\n",
    "    if detect(texte) == \"fr\":\n",
    "        tagger = treetaggerwrapper.TreeTagger(TAGLANG=\"fr\")\n",
    "        tags = tagger.tag_text(texte)\n",
    "    else:\n",
    "        tagger = treetaggerwrapper.TreeTagger(TAGLANG=\"en\")\n",
    "        tags = tagger.tag_text(texte)\n",
    "        lang = \"EN\"\n",
    "    for ligne in tags:\n",
    "        ligne = ligne.strip()\n",
    "        defToken = ligne.split(\"\\t\")\n",
    "        # filtre les tokens vides\n",
    "        if len(defToken) >= 3:\n",
    "            # reduit aux lemmes et pos\n",
    "            token = [defToken[2]]\n",
    "            tokensLemme.append(token)\n",
    "    # renvoie une liste de tokens (lemme et pos)\n",
    "    return tokensLemme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['chômage'], ['salaire'], ['citoyen']]"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatiseTermes(termes_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def lemmatiseTokensRequete(fichier):\n",
    "    tokensLemmePos = []\n",
    "    fiParse = fichier + \".par\"\n",
    "    \n",
    "    # execute TreeTagger\n",
    "    termes_requete = litTexteDuFichier(fichier)\n",
    "    if detect(termes_requete) == \"fr\":\n",
    "        cmd = \"tree-tagger-french\"\n",
    "    else:\n",
    "        cmd = \"tree-tagger-english\"\n",
    "    os.system(\"%s %s > %s\" % (cmd, fichier, fiParse))\n",
    "\n",
    "    # lit le csv produit par treeTagger\n",
    "    with open(fiParse, \"r\") as FI:\n",
    "        for ligne in FI:\n",
    "            ligne = ligne.strip()\n",
    "            defToken = ligne.split(\"\\t\")\n",
    "            # filtre les tokens vides\n",
    "            if len(defToken) >= 3:\n",
    "                if defToken[2] != \"<unknown>\":\n",
    "                    # ajouter les tokens lemmatisés dans la liste \n",
    "                    tokensLemmePos.append(defToken[2])\n",
    "                else:\n",
    "                    tokensLemmePos.append(defToken[0])\n",
    "    # renvoie une liste de tokens (lemme)\n",
    "    return tokensLemmePos"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
